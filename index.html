
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>DIAL</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://instructionaugmentation.github.com/img/dial.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://instructionaugmentation.github.com/"/>
    <meta property="og:title" content="DIAL" />
    <meta property="og:description" content="Project page for Data-driven Instruction Augmentation for Language-conditioned Control." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="DIAL" />
    <meta name="twitter:description" content="Project page for Data-driven Instruction Augmentation for Language-conditioned Control." />
    <meta name="twitter:image" content="https://instructionaugmentation.github.com/img/dial.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Robotic Skill Acquisition via Instruction Augmentation </br>  with Vision-Language Models </br> 
                <small>
                    To Appear at RSS 2023
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
			<a href="https://tedxiao.me/">
                          Ted Xiao*
			</a>
                    </li>
                    <li>
                        <a href="https://harrischan.com/">
                            Harris Chan*
                        </a>
                    </li>
                    <li>
                        <a href="https://sermanet.github.io/">
                          Pierre Sermanet
                        </a>
                    </li>
                    <li>
			<a href="https://ayzaan.com/">
                          Ayzaan Wahid
			</a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?hl=en&user=-NWRNhMAAAAJ">
                          Anthony Brohan
                        </a>
                    </li>
 	            <li>
                        <a href="https://karolhausman.github.io/">
                          Karol Hausman
                        </a>
                    </li><br>
                    <li>
                        <a href="https://people.eecs.berkeley.edu/~svlevine/">
                          Sergey Levine
                        </a>
                    </li>
                    <li>
                    	<a href="https://jonathantompson.github.io/">
                          Jonathan Tompson
                  	</a>
                    </li>
                    <br>
                    <a href="https://research.google/teams/brain/robotics/">
                    <image src="img/robotics-at-google.png" height="40px"> Robotics at Google</a> <br><br>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2211.11736">
                            <image src="img/mip_paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/lbaqjJJMAFg">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Trajectory Videos</strong></h4>
                            </a>
                        </li>
                        <!-- li>
                            <a href="https://ai.googleblog.com/2021/04/multi-task-robotic-reinforcement.html">
                            <image src="img/google-ai-blog-small.png" height="60px">
                                <h4><strong>Blogpost</strong></h4>
                            </a>
                        </li -->
                        <!-- <li>
                            <a href="https://github.com/">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p style="text-align:center;">
<!--        	    <image src="img/collage_v2.gif" class="img-responsive">-->
                </p>
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    In recent years, much progress has been made in learning robotic manipulation policies that follow natural language instructions.
Such methods typically learn from corpora of robot-language data that was either collected with specific tasks in mind or expensively re-labelled by humans with rich language descriptions in hindsight. 
Recently, large-scale pretrained vision-language models (VLMs) like CLIP or ViLD have been applied to robotics for learning representations and scene descriptors.
Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets to make them useful even for tasks that are not reflected in their ground truth annotations? 
For example, if the original annotations contained simple task descriptions such as "pick up the apple", a pretrained VLM-based labeller could significantly expand the number of semantic concepts available in the data and introduce spatial concepts such as "the apple on the right side of the table" or alternative phrasings such as "the red colored fruit".
To accomplish this, we introduce <strong>D</strong>ata-driven <strong>I</strong>nstruction <strong>A</strong>ugmentation for <strong>L</strong>anguage-conditioned control (DIAL): we utilize semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabelled demonstration data and then train language-conditioned policies on the augmented datasets.
This method enables cheaper acquisition of useful language descriptions compared to expensive human labels, allowing for more efficient label coverage of large-scale datasets. 
We apply DIAL to a challenging real-world robotic manipulation domain where 96.5% of the 80,000 demonstrations do not contain crowd-sourced language annotations.
DIAL enables imitation learning policies to acquire new capabilities and generalize to 60 novel instructions unseen in the original dataset.

                </p>
            </div>
        </div>



        <!--div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/i3uUGSko2zY" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            	<br>
                <h3>
                    Approach
                </h3>
                <p class="text-justify">
                DIAL consists of three stages:  (1) finetuning a VLM’s vision and language representation on a small offline dataset of trajectories with crowdsourced episode-level natural language description, (2) generating alternative instructions for a larger offline dataset of trajectories with the VLM, and (3) learning a language-conditioned policy viabehavior-cloning on this augmented offline data.
                </p>
                <p style="text-align:center;">
                    <image src="img/main_dial_figure.png"  class="img-responsive" height="600px">
                </p>
                <p class="text-justify">
                After finetuning CLIP on the portion of the training dataset that contains crowd-sourced language instructions, we can automatically label the rest of the dataset without any additional human effort. In our setting, we finetune CLIP on crowd-sourced human labels for 2,800 demonstrations out of the total training dataseet of 80,000 teleoperated demonstrations.
                </p>
                <p style="text-align:center;">
                    <image src="img/dial_animation.gif"  class="img-responsive" height="600px">
                </p>
                CLIP predicts language instructions by scoring candidate instructions sourced from the original crowd-sourced dataset as well as LLM caption proposals. While many of these candidate instructions might be erroenous or irrelevant, CLIP is often able to predict instruction labels describing skills or concepts not present in the original label.
                <p style="text-align:center;">
                    <image src="img/dial_labeling.gif"  class="img-responsive" height="600px">
                </p>
                <!--
		<p style="test-align:center;">
		    <image src="img/performance-summary.png"  class="image-responsive" width="100%">
		</p>
-->
<!--
		<p style="test-align:center;">
		    <image src="img/bellman-update.png"  class="image-responsive" width="80%">
		</p>
-->
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            	<br>
                <h3>
                    Results
                </h3>       
                <p class="text-justify">
                    In our experiments, we investigate whether DIAL can improve policy performance on 60 novel evaluation tasks. We compare against different instruction augmentation methods, including methods that do not utilize visual context. To study how applicable DIAL is to various practical robot learning scenarios, we consider the setting where we have a fully labeled trajectory dataset as well as where we have only a partially labeled trajectory dataset that contains episodes with no corresponding language labels. We find that DIAL is able to successfully imbue offline datasets with additional semantic concepts not contained in the original instruction set, and demonstrate these capabilities on a large-scale real robotic manipulation setting.
                </p>
                <iframe width="750" height="500"
src="https://www.youtube.com/embed/lbaqjJJMAFg">
                </iframe>
                <p class="text-justify">
                    We perform over 1,300 real world robot policy evaluations on 60 novel instructions that we organize into three categories. "Spatial" tasks focus on instructions involving reasoning about spatial relationships, such as specifying an object’s initial position relative to other objects in the scene. "Rephrased" tasks are linguistic re-phrasings of the original tasks prompted during teleoperation, such as referring to sodas and chips by their colors instead of their brand name. "Semantic" tasks describe skills not contained in the original dataset, such as moving objects away from all other objects, since the original dataset only contains trajectories of moving objects towards other object. We find that DIAL outperforms instruction augmentation baselines across all three categories.
                </p>
                <p style="test-align:center;">
                    <image src="img/results_1.png"  class="image-responsive" width="100%">
                </p>
                <p class="text-justify">
                    The source trajectory dataset we utilize consists of a 5,600 trajectories dataset (Dataset A) with crowdsourced hindsight labels and a larger 80,000 trajectories dataset (Dataset B) that does not have any crowdsourced instructions. Even though Dataset B does not contain hindsight labels, it does contain structured task information that was used to guide human demonstrators as to which task should be collected (for example, “pick coke can”); we refer to these structured commands as foresight instructions. In the previous experiment, we considered all information available during both instruction augmentation as well as policy training. However, does DIAL still improve policy performance in settings where the source dataset is only partially labeled? We study the settings where foresight instructions are not available as well as the setting where crowd-sourced labels are not available. We find that in both cases DIAL significantly increases performance on novel evaluation instructions. This experiment is motivated by the setting where large amounts of unstructured trajectory data are available but hindsight labels are expensive to collect, such as robot play data collection.
                </p>
                <p style="test-align:center;">
                    <image src="img/results_2.png"  class="image-responsive" width="100%">
                </p>
                <p class="text-justify">
                    We also study the tradeoff between relabeled instruction accuracy, augmented dataset size, and downstream policy performance. Emperically, we find that the Finetuned CLIP model we utilize becomes very uncertain after the initial few hindsight label predictions; we propose two instruction prediction methods, and find that the more conservative approach is able to strike a balance between producing rich instruction augmentations while still providing accurate supervision signal for the control policy. For more details on this analysis and experimental results, please refer to the full paper.
                </p>
                <p style="test-align:center;">
                    <image src="img/instructions_example.png"  class="image-responsive" width="100%">
                </p>
	    </div>
        </div>
            
        
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{xiao2022robotic,
  title={Robotic Skill Acquistion via Instruction Augmentation with Vision-Language Models},
  author={Xiao, Ted and Chan, Harris and Sermanet, Pierre and Wahid, Ayzaan and Brohan, Anthony and Hausman, Karol and Levine, Sergey and Tompson, Jonathan},
  booktitle={Proceedings of Robotics: Science and Systems},
  year={2023}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The authors would like to thank Kanishka Rao, Debidatta Dwibedi, Pete Florence, Yevgen Chebotar, Fei Xia, and Corey Lynch for valuable feedback and discussions. We would also like to thank Emily Perez, Dee M,
Clayton Tan, Jaspiar Singh, Jornell Quiambao, and Noah Brown for navigating the ever-changing
challenges of data collection and robot policy evaluation at scale. Additionally, Tom Small designed informative animations to visualize DIAL. Finally, we would like to thank the
large team that built SayCan, upon which we develop DIAL.
                    <br><br>
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
